from collections import defaultdict
from typing import Dict, Generator, Iterable, List, Union

from instances import CASE_TYPES, TARGET_CASES, PasEvaluationInstance, PasOutputForInference
from iterators import PasBucketIterator
from logzero import logger
from models import AsModel, AsPzeroModel
from tqdm import tqdm


def compute_pas_f_score(eval_instances: Iterable[PasEvaluationInstance]):
    """Evaluation of the model prediction on the train/dev set

    Args:
        eval_instances (Iterable[PasEvaluationInstance]): the instances to be used for evaluation
    Returns:
        f (float): f1 score of dep, intra, inter
        exo_f (float): f1 score of exophoric
    """
    results = {
        "intra": {case_name: defaultdict(int) for case_name in TARGET_CASES},
        "inter": {case_name: defaultdict(int) for case_name in TARGET_CASES},
        "exo": {case_name: defaultdict(int) for case_name in TARGET_CASES},
        "dep": {case_name: defaultdict(int) for case_name in TARGET_CASES},
    }

    summary_results = {
        "zar": defaultdict(int),
        "all": defaultdict(int),
    }

    for eval_inst in eval_instances:
        for predict, gold, case_name, case_type in zip(
            eval_inst["predicts"], eval_inst["golds"], eval_inst["case_names"], eval_inst["case_types"]
        ):
            if gold[0] <= 0:
                continue

            _compute_f_factor(predict, gold, results[case_type][case_name])

        for predict, exo_predict, gold, case_name, case_type in zip(
            eval_inst["predicts"],
            eval_inst["exo_predicts"],
            eval_inst["exo_golds"],
            eval_inst["case_names"],
            eval_inst["case_types"],
        ):
            if gold == -100:
                continue

            if predict != 0:
                exo_predict = predict

            _compute_f_factor(exo_predict, [gold], results["exo"][case_name])

    for case_type in results.keys():
        results[case_type]["all"] = defaultdict(int)
        for case_name in TARGET_CASES:
            for factor in ["pp", "np", "pn", "nn"]:
                results[case_type]["all"][factor] += results[case_type][case_name][factor]
                summary_results["all"][factor] += results[case_type][case_name][factor]

                if case_type in ["intra", "inter", "exo"]:
                    summary_results["zar"][factor] += results[case_type][case_name][factor]

    logger.info("**** Score ****")
    f_score = defaultdict(int)
    for case_type in results.keys():
        logger.info("---- {} ----".format(case_type))
        _compute_f_score_from_factor(results[case_type]["all"])
        f_score[case_type] = results[case_type]["all"]["f1"]
        logger.info("* F1 Score: {}".format(f_score[case_type]))

    for case_type in summary_results.keys():
        logger.info("---- {} ----".format(case_type))
        _compute_f_score_from_factor(summary_results[case_type])
        f_score[case_type] = summary_results[case_type]["f1"]
        logger.info("* F1 Score: {}".format(f_score[case_type]))

    return f_score["all"], f_score["zar"]


def compute_pzero_f_score(predicts: List[int], golds: List[List[int]]) -> float:
    """Compute the f1 score"""
    results = defaultdict(int)
    for predict, gold in zip(predicts, golds):
        _compute_f_factor(predict, gold, results)
    f1 = _compute_f_score_from_factor(results)

    return f1


def _compute_f_factor(predict: int, gold: List[int], results: Dict[str, int]) -> None:
    """Compute the value for calculating the f1 score"""
    # if gold label is [0], it indicates that there is no correct answer in input sentences.
    if gold == [0]:
        if predict == 0:
            results["nn"] += 1
        else:
            results["np"] += 1
    elif predict != 0:
        if predict in gold:
            results["pp"] += 1
        else:
            results["np"] += 1
            results["pn"] += 1
    else:
        results["pn"] += 1


def _compute_f_score_from_factor(results: Dict[str, Union[int, float]]) -> float:
    """Compute the f1 score from values generated by 'calc_f_factor'"""
    p_p, n_p, p_n, n_n = results["pp"], results["np"], results["pn"], results["nn"]
    prec = p_p / (p_p + n_p) if p_p > 0 else 0
    rec = p_p / (p_p + p_n) if p_p > 0 else 0
    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0

    log_format = "Precision: {:.4f},\tRecall: {:.4f},\tF1: {:.4f},\tPP: {},\tNP: {},\tPN: {},\tNN: {}"
    logger.info(log_format.format(prec, rec, f1, p_p, n_p, p_n, n_n))

    results["prec"], results["rec"], results["f1"] = prec, rec, f1

    return f1


def generate_pas_evaluation_instance(
    model: Union[AsModel, AsPzeroModel],
    data_loader: PasBucketIterator,
) -> Generator[PasEvaluationInstance, None, None]:
    """Generate instances for evaluation

    Args:
        model (Union[ASModel, AsPzeroModel]): the model for inference
        data_loader (PasBucketIterator): data loader
    Return:
        eval_instance (PasEvaluationInstance)
    """
    data_loader.create_batches()
    for batch in tqdm(data_loader):
        batch_size: int = len(batch["input_ids"])
        output: PasOutputForInference = model.inference(batch)

        predicts = output["predicts"]
        exo_predicts = output["exo_predicts"]

        if isinstance(model, AsPzeroModel):
            golds = batch["gold_positions"]
            exo_golds = batch["exo_ids"]
            case_names = batch["case_names"]
            case_types = batch["case_types"]
            eval_infos = batch["eval_info"]

        elif isinstance(model, AsModel):
            golds = [gold[case_name] for gold in batch["gold_positions"] for case_name in TARGET_CASES]
            exo_golds = [exo_gold[case_name] for exo_gold in batch["exo_ids"] for case_name in TARGET_CASES]
            case_names = [case_name for _ in range(batch_size) for case_name in TARGET_CASES]
            case_types = [case_type for _ in range(batch_size) for case_type in CASE_TYPES]
            eval_infos = [info for info in batch["eval_info"] for _ in range(3)]

        else:
            raise ValueError(f"unsupported type of the model: {type(model)}")

        assert (
            len(predicts)
            == len(exo_predicts)
            == len(golds)
            == len(exo_golds)
            == len(case_names)
            == len(case_types)
            == len(eval_infos)
        )

        eval_instance = PasEvaluationInstance(
            predicts=predicts,
            exo_predicts=exo_predicts,
            golds=golds,
            exo_golds=exo_golds,
            case_names=case_names,
            case_types=case_types,
            eval_infos=eval_infos,
        )

        yield eval_instance
